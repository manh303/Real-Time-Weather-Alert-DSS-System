# docker/config/airflow/airflow.cfg

[core]
# The home folder for airflow, default is ~/airflow
airflow_home = /opt/airflow

# The folder where your airflow pipelines live
dags_folder = /opt/airflow/dags

# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3, Google Cloud Storage or Elastic Search
remote_logging = False

# The executor class that airflow should use
executor = CeleryExecutor

# The SqlAlchemy connection string to the metadata database
sql_alchemy_conn = postgresql+psycopg2://airflow:airflow@postgres-airflow/airflow

# The amount of parallelism as a setting to the executor
parallelism = 32

# The number of task instances allowed to run concurrently by the scheduler
dag_concurrency = 16

# Are DAGs paused by default at creation
dags_are_paused_at_creation = True

# When not using pools, tasks are run in the "default pool"
non_pooled_task_slot_count = 128

# The maximum number of active DAG runs per DAG
max_active_runs_per_dag = 16

# Whether to load the DAG examples that ship with Airflow
load_examples = False

# Where your Airflow plugins are stored
plugins_folder = /opt/airflow/plugins

# Secret key to save connection passwords in the db
fernet_key = your_fernet_key_here_change_this_in_production

# Whether to disable pickling dags
donot_pickle = False

# How long before timing out a python file import
dagbag_import_timeout = 30

# The class to use for running task instances in a subprocess
task_runner = StandardTaskRunner

# If set, tasks without a `run_as_user` argument will be run with this user
default_impersonation = 

# What security module to use (for example kerberos)
security = 

# Turn unit test mode on (overwrites many configuration options with test values)
unit_test_mode = False

# Whether to enable pickling for XCom (note that this is insecure and allows for RCE exploits)
enable_xcom_pickling = False

# When a task is killed, this defines the time in seconds that it has to cleanup after receiving a SIGTERM, before it is SIGKILLED
killed_task_cleanup_time = 60

[logging]
# The folder where airflow should store its log files
base_log_folder = /opt/airflow/logs

# Airflow can store logs remotely in AWS S3 or Google Cloud Storage
remote_logging = False

# Logging level
logging_level = INFO

# Logging class
logging_config_class = 

# Log format for tasks
log_format = [%%(asctime)s] {%%(filename)s:%%(lineno)d} %%(levelname)s - %%(message)s
simple_log_format = %%(asctime)s %%(levelname)s - %%(message)s

[metrics]
statsd_on = False
statsd_host = localhost
statsd_port = 8125
statsd_prefix = airflow

[webserver]
# The base url of your website as airflow cannot guess what domain or
base_url = http://localhost:8082

# Default timezone to display all dates in the UI
default_ui_timezone = UTC

# The ip specified when starting the web server
web_server_host = 0.0.0.0

# The port on which to run the web server
web_server_port = 8080

# Paths to the SSL certificate and key for the web server
web_server_ssl_cert = 
web_server_ssl_key = 

# Number of seconds the webserver waits before killing gunicorn master
web_server_master_timeout = 120

# Number of seconds the gunicorn webserver waits before timing out on a worker
web_server_worker_timeout = 120

# Number of workers to run the Gunicorn web server
workers = 4

# The worker class gunicorn should use
worker_class = sync

# Log files for the gunicorn webserver
access_logfile = -
error_logfile = -

# Expose the configuration file in the web server
expose_config = False

# Set to true to turn on authentication
authenticate = False

# Filter the list of dags by owner name (requires authentication to be enabled)
filter_by_owner = False

# Filtering mode
owner_mode = user

[email]
email_backend = airflow.utils.email.send_email_smtp

[smtp]
# If you want airflow to send emails on retries, failure, and you want to use
smtp_host = localhost
smtp_starttls = True
smtp_ssl = False
smtp_port = 587
smtp_mail_from = airflow@example.com

[celery]
# This section only applies if you are using the CeleryExecutor
celery_app_name = airflow.executors.celery_executor
broker_url = redis://:@redis:6379/0
result_backend = db+postgresql://airflow:airflow@postgres-airflow/airflow

# Celery Flower is a sweet UI for Celery
flower_host = 0.0.0.0
flower_port = 5555

# Default queue that tasks get assigned to and that worker listen on
default_queue = default

# The concurrency that will be used when starting workers with the
celery_concurrency = 16

# When you start an airflow worker, airflow starts a tiny web server
worker_log_server_port = 8793

[scheduler]
# Task instances listen for external kill signal (when you `airflow tasks kill`),
catchup_by_default = False

# The scheduler constantly tries to trigger new tasks (look at the
scheduler_heartbeat_sec = 5

# Number of seconds after which a DAG file is parsed
min_file_process_interval = 0

# How often (in seconds) to scan the DAGs directory for new files
dag_dir_list_interval = 300

# How often should stats be printed to the logs
print_stats_interval = 30

# If the last scheduler heartbeat happened more than scheduler_health_check_threshold ago (in seconds), scheduler is considered unhealthy
scheduler_health_check_threshold = 30

# The number of times to try to schedule each DAG file
schedule_after_task_execution = True

# Turn off scheduler use of cron intervals by setting this to False
use_job_schedule = True

# Allow the scheduler to use the raw SQL to schedule DAGs
allow_trigger_in_future = False