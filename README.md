# ğŸŒ¦ï¸ Real-Time Weather Alert DSS System

> A Big Data Decision Support System (DSS) for monitoring and alerting extreme weather conditions in Vietnam using real-time data.

---

## ğŸ“Œ Project Overview

This project builds a **real-time decision support system (DSS)** that collects, processes, and stores weather data across 63 provinces in Vietnam. It alerts users about extreme weather events like **heatwaves** (temperature â‰¥ 37Â°C) and **drought conditions** (humidity â‰¤ 25%).

### ğŸ”¥ Key Features:
- Collect real-time weather data from OpenWeatherMap API
- Stream data using **Apache Kafka**
- Process and filter critical data using **Apache Spark Streaming**
- Store raw and filtered data in **MongoDB**
- Automate the workflow using **Apache Airflow**
- Containerized deployment with **Docker Compose**

---

## ğŸ¯ Project Objectives

- Build a scalable architecture using Big Data technologies
- Demonstrate real-time stream processing and alerting
- Provide a foundation for weather-based decision making and forecasting

---

## ğŸ§° Technologies Used

| Component         | Technology                |
|------------------|---------------------------|
| Data Ingestion    |  OpenWeatherMap API |
| Message Queue     | Apache Kafka              |
| Stream Processing | Apache Spark Structured Streaming |
| NoSQL Storage     | MongoDB                   |
| Orchestration     | Apache Airflow            |
| Containerization  | Docker + Docker Compose   |
| (Optional) DWH     | PostgreSQL                |
| (Optional) Dashboard | Flask and React|

---

## ğŸ“ Project Structure
weather-alert-dss/
â”œâ”€â”€ airflow/ # DAGs & configs for Airflow pipeline
â”œâ”€â”€ docker/ # Docker config files
â”‚ â”œâ”€â”€ docker-compose.yml
â”‚ â””â”€â”€ .env
â”œâ”€â”€ spark-app/ # Spark Streaming processing scripts
â”‚ â””â”€â”€ spark_streaming_processor.py
â”œâ”€â”€ ingestion/ # Weather data collection scripts
â”‚ â””â”€â”€ crawl_weather_kafka.py
â”œâ”€â”€ data/ # Mount points for MongoDB and output logs
â”‚ â””â”€â”€ db/
â”œâ”€â”€ jars/ # Kafka-Spark JARs
â”‚ â””â”€â”€ spark-sql-kafka-0-10_2.12-3.3.2.jar
â”œâ”€â”€ dags/ # Airflow DAG scripts
â”‚ â””â”€â”€ weather_etl_dag.py
â”œâ”€â”€ output/ # Exported logs, charts, PDFs
â”œâ”€â”€ requirements.txt # Python dependencies
â”œâ”€â”€ README.md # Project documentation
â””â”€â”€ .env # Environment variables

1. **Ingestion**  
   Script `crawl_weather_kafka.py` calls OpenWeatherMap API for 63 provinces and pushes results into Kafka topic `weather_data`.

2. **Streaming Processing**  
   `spark_streaming_processor.py` consumes Kafka stream, parses JSON data, and filters alerts (e.g., temp â‰¥ 37Â°C).

3. **Storage**  
   - All data is saved into MongoDB.
   - Can be extended to store into PostgreSQL for reporting.

4. **Orchestration**  
   - Apache Airflow schedules the crawling & processing every N minutes.
   - DAG `weather_etl_dag.py` automates the full pipeline.
